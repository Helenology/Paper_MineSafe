{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42cb8a1b-ce32-4edc-a296-f15c3dd4fedf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, error: 16.320908213046007\n",
      "iteration: 2, error: 7.34465539868624\n",
      "iteration: 3, error: 4.485678942986565\n",
      "iteration: 4, error: 2.9793070397594628\n",
      "iteration: 5, error: 2.2528348579069952\n",
      "iteration: 6, error: 1.7025888310159714\n",
      "iteration: 7, error: 1.293094556967247\n",
      "iteration: 8, error: 1.0354972425311202\n",
      "iteration: 9, error: 0.8416237194625962\n",
      "iteration: 10, error: 0.7036348017465519\n",
      "iteration: 11, error: 0.6069763587327122\n",
      "iteration: 12, error: 0.5316106520152405\n",
      "iteration: 13, error: 0.4693515164089797\n",
      "iteration: 14, error: 0.4167852580215523\n",
      "iteration: 15, error: 0.3711894223881126\n",
      "iteration: 16, error: 0.33057362312763006\n",
      "iteration: 17, error: 0.2952373796441481\n",
      "iteration: 18, error: 0.26587585059644714\n",
      "iteration: 19, error: 0.24158035179223314\n",
      "iteration: 20, error: 0.22136892121588367\n",
      "iteration: 21, error: 0.2047793476561704\n",
      "iteration: 22, error: 0.19082802245594227\n",
      "iteration: 23, error: 0.17859210791655905\n",
      "iteration: 24, error: 0.16788449622852994\n",
      "iteration: 25, error: 0.15817745126933389\n",
      "iteration: 26, error: 0.14926012569620817\n",
      "iteration: 27, error: 0.14109145908301046\n",
      "iteration: 28, error: 0.1335507661510675\n",
      "iteration: 29, error: 0.12692133998914093\n",
      "iteration: 30, error: 0.12072352312965207\n",
      "iteration: 31, error: 0.11484897919898587\n",
      "iteration: 32, error: 0.10977497896720563\n",
      "iteration: 33, error: 0.10501058863197112\n",
      "iteration: 34, error: 0.10062323717196864\n",
      "iteration: 35, error: 0.09640627447683986\n",
      "iteration: 36, error: 0.09236809779065018\n",
      "iteration: 37, error: 0.08880693237556547\n",
      "iteration: 38, error: 0.08567746854401102\n",
      "iteration: 39, error: 0.08280126166810035\n",
      "iteration: 40, error: 0.08013880443234157\n",
      "iteration: 41, error: 0.07762263376020981\n",
      "iteration: 42, error: 0.07518440002981913\n",
      "iteration: 43, error: 0.07290951492087423\n",
      "iteration: 44, error: 0.07056358509360942\n",
      "iteration: 45, error: 0.06840614120381167\n",
      "iteration: 46, error: 0.06634829200369215\n",
      "iteration: 47, error: 0.0643899165430248\n",
      "iteration: 48, error: 0.06253971925870144\n",
      "iteration: 49, error: 0.06089775372354206\n",
      "iteration: 50, error: 0.05936623071549921\n",
      "iteration: 51, error: 0.05785415475895498\n",
      "iteration: 52, error: 0.05647273817847604\n",
      "iteration: 53, error: 0.05515076721168171\n",
      "iteration: 54, error: 0.05390004240992724\n",
      "iteration: 55, error: 0.052706438429043226\n",
      "iteration: 56, error: 0.05145366201918662\n",
      "iteration: 57, error: 0.05028254852458873\n",
      "iteration: 58, error: 0.049147941795349995\n",
      "iteration: 59, error: 0.04802163189895681\n",
      "iteration: 60, error: 0.046939089707799954\n",
      "iteration: 61, error: 0.0459231022047978\n",
      "iteration: 62, error: 0.04496023947700571\n",
      "iteration: 63, error: 0.04405113130233796\n",
      "iteration: 64, error: 0.04316901562401184\n",
      "iteration: 65, error: 0.042364110239035434\n",
      "iteration: 66, error: 0.04158598844395757\n",
      "iteration: 67, error: 0.04080777793040497\n",
      "iteration: 68, error: 0.040061646087389234\n",
      "iteration: 69, error: 0.03932593794206681\n",
      "iteration: 70, error: 0.03863353400243265\n",
      "iteration: 71, error: 0.03796446766560402\n",
      "iteration: 72, error: 0.03725780662448124\n",
      "iteration: 73, error: 0.03659740617139206\n",
      "iteration: 74, error: 0.03595823551344207\n",
      "iteration: 75, error: 0.035351188185658446\n",
      "iteration: 76, error: 0.034792093884853\n",
      "iteration: 77, error: 0.03422668533593837\n",
      "iteration: 78, error: 0.033679682126185995\n",
      "iteration: 79, error: 0.033128376579475864\n",
      "iteration: 80, error: 0.03260637048560364\n",
      "iteration: 81, error: 0.03212619831216933\n",
      "iteration: 82, error: 0.03164745798736982\n",
      "iteration: 83, error: 0.03119178527285733\n",
      "iteration: 84, error: 0.030753274598281165\n",
      "iteration: 85, error: 0.030320159570007516\n",
      "iteration: 86, error: 0.02991242307121991\n",
      "iteration: 87, error: 0.02951781688721545\n",
      "iteration: 88, error: 0.029123379866864668\n",
      "iteration: 89, error: 0.028726428599907223\n",
      "iteration: 90, error: 0.02835745975192847\n",
      "iteration: 91, error: 0.027991346623121933\n",
      "iteration: 92, error: 0.02764510557742651\n",
      "iteration: 93, error: 0.027284511480696515\n",
      "iteration: 94, error: 0.02693426922454305\n",
      "iteration: 95, error: 0.026524917562743153\n",
      "iteration: 96, error: 0.026185301407896138\n",
      "iteration: 97, error: 0.02587388913272895\n",
      "iteration: 98, error: 0.025569888483391874\n",
      "iteration: 99, error: 0.025277355449023663\n",
      "iteration: 100, error: 0.024984099697023376\n"
     ]
    }
   ],
   "source": [
    "## Training codes for LRPCA \n",
    "## Implemented by Jialin Liu @ Alibaba DAMO\n",
    "## Date: Dec. 07, 2021\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.sparse.linalg as lina\n",
    "import time\n",
    "# RPCA 代码\n",
    "import sys\n",
    "sys.path.append(\"/mnt/MineSafe-2024/models/robust-pca-master\")\n",
    "from r_pca import R_pca\n",
    "\n",
    "## ================Preparations====================\n",
    "device = torch.device('cuda:0')\n",
    "datatype = torch.float64\n",
    "Y0_t = np.load(\"./raw_batch.npy\")  # observation\n",
    "\n",
    "## =============Generate RPCA problems=============\n",
    "rpca = R_pca(Y0_t)          # according to Appendix C, run classic RPCA to obtain X\n",
    "bg, fg = rpca.fit(max_iter=100, iter_print=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337a4799-e350-40a7-a809-49a4ddda766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y0_t = torch.from_numpy(Y0_t).to(device)\n",
    "X0_t = torch.from_numpy(bg).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f3b667-dd39-43f0-a65f-b88726bb846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ================Parameters======================\n",
    "r \t\t\t\t= 2\t\t# underlying rank\n",
    "d1 \t\t\t\t= Y0_t.shape[0]\t\t# size (num. of rows)\n",
    "d2 \t\t\t\t= Y0_t.shape[1]\t\t# size (num. of columns)\n",
    "# alpha \t\t\t= 0.1\t\t# fraction of outliers\n",
    "step_initial \t= 0.5\t\t# initial value of step size (eta in the paper)\n",
    "ths_initial \t= 0.25\t\t# initial value of thresholds (zeta in the paper) 1e-3\n",
    "maxIt \t\t\t= 5\t\t# num. of layers you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3de7855-ff87-4b9d-8178-fed7c7215e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ===================LRPCA model===================\n",
    "class MatNet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(type(self),self).__init__()\n",
    "\t\tself.ths_v \t\t= [nn.Parameter(Variable(torch.tensor(ths_initial, dtype=datatype, device = device), requires_grad=True)) for t in range(maxIt)]\n",
    "\t\tself.step \t\t= [nn.Parameter(Variable(torch.tensor(step_initial, dtype=datatype, device = device), requires_grad=True)) for t in range(maxIt)]\n",
    "\t\tself.ths_backup\t= [torch.tensor(ths_initial, dtype=datatype, device = device) for t in range(maxIt)]\n",
    "\n",
    "\tdef thre(self, inputs, threshold):\n",
    "\t\tout = torch.sign(inputs) * torch.max( torch.abs(inputs) - threshold, torch.zeros([1, 1], dtype=datatype, device=device) )\n",
    "\t\treturn out\n",
    "\n",
    "\tdef forward(self, Y0_t, r, X0_t, num_l):\n",
    "\t\t## Initialization\n",
    "\t\tS_t = self.thre(Y0_t, self.ths_v[0])\n",
    "\t\tL, Sigma, R = torch.svd_lowrank(Y0_t - S_t, q = r, niter = 4)\n",
    "\t\tSigsqrt = torch.diag(torch.sqrt(Sigma))\n",
    "\t\tU_t = torch.mm(L, Sigsqrt)\n",
    "\t\tV_t = torch.mm(R, Sigsqrt) \n",
    "\n",
    "        ## Main Loop in LRPCA\n",
    "\t\tfor t in range(1, num_l):\n",
    "\t\t\tYmUV = Y0_t - torch.mm(U_t, V_t.t())\n",
    "\t\t\tS_t = self.thre(YmUV, self.ths_v[t])\n",
    "\t\t\tE_t = YmUV - S_t \n",
    "\t\t\tVkernel = torch.inverse(V_t.t() @ V_t)\n",
    "\t\t\tUkernel = torch.inverse(U_t.t() @ U_t)\n",
    "\t\t\tUnew = U_t + self.step[t] * (torch.mm(E_t,V_t) @ Vkernel)\n",
    "\t\t\tVnew = V_t + self.step[t] * (torch.mm(U_t.t(),E_t).t() @ Ukernel)\n",
    "\t\t\tU_t = Unew\n",
    "\t\t\tV_t = Vnew\n",
    "\n",
    "\t\t## loss function in training\n",
    "\t\tloss = (torch.mm(U_t, V_t.t()) - X0_t).norm() \t\t\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef InitializeThs(self, en_l):\n",
    "\t\tself.ths_v[en_l].data = torch.clone(self.ths_v[en_l-1].data * 0.1)\n",
    "\t\t\n",
    "\tdef CheckNegative(self):\n",
    "\t\tisNegative = False;\n",
    "\t\tfor t in range(maxIt):\n",
    "\t\t\tif(self.ths_v[t].data < 0):\n",
    "\t\t\t\tisNegative = True;\n",
    "\t\tif(isNegative):\n",
    "\t\t\tfor t in range(maxIt):\n",
    "\t\t\t\tself.ths_v[t].data = torch.clone(self.ths_backup[t])\n",
    "\t\telse:\n",
    "\t\t\tfor t in range(maxIt):\n",
    "\t\t\t\tself.ths_backup[t] = torch.clone(self.ths_v[t].data)\n",
    "\t\treturn isNegative;\n",
    "    \n",
    "\tdef CheckNa(self):\n",
    "\t\tisNa = False;\n",
    "\t\tfor t in range(maxIt):\n",
    "\t\t\tif(torch.isnan(self.ths_v[t].data)):\n",
    "\t\t\t\tisNa = True;\n",
    "\t\tif(isNa):\n",
    "\t\t\tfor t in range(maxIt):\n",
    "\t\t\t\tself.ths_v[t].data = torch.clone(self.ths_backup[t])\n",
    "\t\telse:\n",
    "\t\t\tfor t in range(maxIt):\n",
    "\t\t\t\tself.ths_backup[t] = torch.clone(self.ths_v[t].data)\n",
    "\t\treturn isNa;\n",
    "\n",
    "\tdef EnableSingleLayer(self,en_l):\n",
    "\t\tfor t in range(maxIt): \n",
    "\t\t\tself.ths_v[t].requires_grad = False\n",
    "\t\t\tself.step[t].requires_grad = False\n",
    "\t\tself.ths_v[en_l].requires_grad = True\n",
    "\t\tself.step[en_l].requires_grad = True\n",
    "\n",
    "\tdef EnableLayers(self, num_l):\n",
    "\t\tfor t in range(num_l): \n",
    "\t\t\tself.ths_v[t].requires_grad = True\n",
    "\t\t\tself.step[t].requires_grad = True\n",
    "\t\tfor t in range(num_l,maxIt): \n",
    "\t\t\tself.ths_v[t].requires_grad = False\n",
    "\t\t\tself.step[t].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a764d80-9c57-4fbc-b749-0b8aed357944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c554afe-924f-4249-a57e-9860a3915668",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## =================Training Scripts======================\n",
    "Nepoches_pre \t= 500\n",
    "Nepoches_full \t= 1000\n",
    "lr_fac \t\t\t= 1.0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# basic learning rate\n",
    "\n",
    "net = MatNet()\n",
    "optimizers = []\n",
    "\n",
    "for i in range(maxIt):\n",
    "    optimizer = optim.SGD({net.ths_v[i]},lr = lr_fac * ths_initial / 5000.0)\t# optimizer for each layer\n",
    "    optimizer.add_param_group({'params': [net.step[i]], 'lr': lr_fac * 0.1})\t# learning rate for each layer\n",
    "    optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5322be7f-6a25-44ba-a2ce-2d9636434092",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer  0 , Pre-training ======================\n",
      "epoch: 0 \t loss: 1715.2716108542072\n",
      "epoch: 20 \t loss: 8.043741010227187\n",
      "epoch: 40 \t loss: 8.05927368912403\n",
      "epoch: 60 \t loss: 8.0715079170919\n",
      "epoch: 80 \t loss: 8.05557473603394\n",
      "epoch: 100 \t loss: 8.060268404114792\n",
      "epoch: 120 \t loss: 8.060948836586247\n",
      "epoch: 140 \t loss: 8.065619608440155\n",
      "epoch: 160 \t loss: 8.055761015182192\n",
      "epoch: 180 \t loss: 8.06553627171656\n",
      "epoch: 200 \t loss: 8.036135614394958\n",
      "epoch: 220 \t loss: 8.054975392923534\n",
      "epoch: 240 \t loss: 8.069779225933932\n",
      "epoch: 260 \t loss: 8.06703276730379\n",
      "epoch: 280 \t loss: 8.066606532697948\n",
      "epoch: 300 \t loss: 8.07282334565985\n",
      "epoch: 320 \t loss: 8.0681741772168\n",
      "epoch: 340 \t loss: 8.065878918405904\n",
      "epoch: 360 \t loss: 8.064734027005109\n",
      "epoch: 380 \t loss: 8.06275681190323\n",
      "epoch: 400 \t loss: 8.065829780036035\n",
      "epoch: 420 \t loss: 8.068415189247478\n",
      "epoch: 440 \t loss: 8.065061928148056\n",
      "epoch: 460 \t loss: 8.112580810276807\n",
      "epoch: 480 \t loss: 8.060910422428272\n",
      "Layer  0 , Full-training =====================\n",
      "Layer  1 , Pre-training ======================\n",
      "epoch: 0 \t loss: 8.065239681649796\n",
      "epoch: 20 \t loss: 8.06431513817433\n",
      "epoch: 40 \t loss: 8.062731641611853\n",
      "epoch: 60 \t loss: 8.060946851689483\n",
      "epoch: 80 \t loss: 8.064306483379962\n",
      "epoch: 100 \t loss: 8.063894907261968\n",
      "epoch: 120 \t loss: 8.066002877337251\n",
      "epoch: 140 \t loss: 8.064587483142558\n",
      "epoch: 160 \t loss: 8.068269090655612\n",
      "epoch: 180 \t loss: 8.06482965117966\n",
      "epoch: 200 \t loss: 8.067482382469603\n",
      "epoch: 220 \t loss: 8.065557105764231\n",
      "epoch: 240 \t loss: 8.06560207538588\n",
      "epoch: 260 \t loss: 8.06380443254741\n",
      "epoch: 280 \t loss: 8.06378609889871\n",
      "epoch: 300 \t loss: 8.065757239140444\n",
      "epoch: 320 \t loss: 8.063651966429543\n",
      "epoch: 340 \t loss: 8.066060268386384\n",
      "epoch: 360 \t loss: 8.064085292365148\n",
      "epoch: 380 \t loss: 8.064058152712494\n",
      "epoch: 400 \t loss: 8.06660775344836\n",
      "epoch: 420 \t loss: 8.062575990192745\n",
      "epoch: 440 \t loss: 8.059718195444566\n",
      "epoch: 460 \t loss: 8.06754799234538\n",
      "epoch: 480 \t loss: 8.041836883784649\n",
      "Layer  1 , Full-training =====================\n",
      "epoch: 0 \t loss: 8.065948295616213\n",
      "epoch: 20 \t loss: 8.06356444392625\n",
      "epoch: 40 \t loss: 8.066470621367124\n",
      "epoch: 60 \t loss: 8.063157935635404\n",
      "epoch: 80 \t loss: 8.06469189262165\n",
      "epoch: 100 \t loss: 8.07202599205228\n",
      "epoch: 120 \t loss: 8.071681008976848\n",
      "epoch: 140 \t loss: 8.064782991488938\n",
      "epoch: 160 \t loss: 8.063892334071262\n",
      "epoch: 180 \t loss: 8.063278811443721\n",
      "epoch: 200 \t loss: 8.066782261830737\n",
      "epoch: 220 \t loss: 8.0684842785224\n",
      "epoch: 240 \t loss: 8.055657521908643\n",
      "epoch: 260 \t loss: 7.996528462758174\n",
      "epoch: 280 \t loss: 8.064338845960073\n",
      "epoch: 300 \t loss: 8.061630756652587\n",
      "epoch: 320 \t loss: 8.043924370801859\n",
      "epoch: 340 \t loss: 8.063896168878003\n",
      "epoch: 360 \t loss: 8.067915558652649\n",
      "epoch: 380 \t loss: 8.064665493751006\n",
      "epoch: 400 \t loss: 8.077646261560124\n",
      "epoch: 420 \t loss: 8.062927328899766\n",
      "epoch: 440 \t loss: 8.063064224979723\n",
      "epoch: 460 \t loss: 8.065762533805001\n",
      "epoch: 480 \t loss: 8.065030709089365\n",
      "epoch: 500 \t loss: 8.063594879869969\n",
      "epoch: 520 \t loss: 8.064291365946346\n",
      "epoch: 540 \t loss: 8.066302088907827\n",
      "epoch: 560 \t loss: 8.114939246944854\n",
      "epoch: 580 \t loss: 8.068502509769257\n",
      "epoch: 600 \t loss: 8.06431255963749\n",
      "epoch: 620 \t loss: 8.064402276561022\n",
      "epoch: 640 \t loss: 8.061823698903105\n",
      "epoch: 660 \t loss: 8.063829552959625\n",
      "epoch: 680 \t loss: 8.05816293468752\n",
      "epoch: 700 \t loss: 8.063598874166209\n",
      "epoch: 720 \t loss: 8.0653747011589\n",
      "epoch: 740 \t loss: 8.065181285746506\n",
      "epoch: 760 \t loss: 8.063067839473653\n",
      "epoch: 780 \t loss: 8.065733692714735\n",
      "epoch: 800 \t loss: 8.064204396310144\n",
      "epoch: 820 \t loss: 8.063406471495863\n",
      "epoch: 840 \t loss: 7.606559156672267\n",
      "epoch: 860 \t loss: 8.066018473161002\n",
      "epoch: 880 \t loss: 7.99263656158852\n",
      "epoch: 900 \t loss: 8.062826789794572\n",
      "epoch: 920 \t loss: 8.063367724258134\n",
      "epoch: 940 \t loss: 8.06927170939312\n",
      "epoch: 960 \t loss: 8.069159853276277\n",
      "epoch: 980 \t loss: 8.033702216783883\n",
      "Layer  2 , Pre-training ======================\n",
      "epoch: 0 \t loss: 7.841856097860036\n",
      "epoch: 20 \t loss: 7.605066316888657\n",
      "epoch: 40 \t loss: 7.518603240668941\n",
      "epoch: 60 \t loss: 7.465637484817981\n",
      "epoch: 80 \t loss: 7.458872519560475\n",
      "epoch: 100 \t loss: 7.453872756991809\n",
      "epoch: 120 \t loss: 7.448883055182504\n",
      "epoch: 140 \t loss: 7.449878808103408\n",
      "epoch: 160 \t loss: 7.444787481681354\n",
      "epoch: 180 \t loss: 7.447097786287497\n",
      "epoch: 200 \t loss: 7.444401399536925\n",
      "epoch: 220 \t loss: 7.464317868124889\n",
      "epoch: 240 \t loss: 7.456000128627702\n",
      "epoch: 260 \t loss: 7.451498772573114\n",
      "epoch: 280 \t loss: 7.445700708271833\n",
      "epoch: 300 \t loss: 7.44393563282832\n",
      "epoch: 320 \t loss: 7.484741577780665\n",
      "epoch: 340 \t loss: 7.448039556023407\n",
      "epoch: 360 \t loss: 7.448847088516525\n",
      "epoch: 380 \t loss: 7.446250039646648\n",
      "epoch: 400 \t loss: 7.448883704617958\n",
      "epoch: 420 \t loss: 7.434198647776417\n",
      "epoch: 440 \t loss: 7.450059465920035\n",
      "epoch: 460 \t loss: 7.44644184059183\n",
      "epoch: 480 \t loss: 7.44613871439302\n",
      "Layer  2 , Full-training =====================\n",
      "epoch: 0 \t loss: 7.4330036208670505\n",
      "epoch: 20 \t loss: 7.273758904475085\n",
      "epoch: 40 \t loss: 7.204125602961822\n",
      "epoch: 60 \t loss: 7.2517668841062015\n",
      "epoch: 80 \t loss: 7.248700820324914\n",
      "epoch: 100 \t loss: 7.267601345092874\n",
      "epoch: 120 \t loss: 7.247310055175567\n",
      "epoch: 140 \t loss: 7.247220633712495\n",
      "epoch: 160 \t loss: 7.242930384184321\n",
      "epoch: 180 \t loss: 7.243527734300942\n",
      "epoch: 200 \t loss: 7.2451052363469595\n",
      "epoch: 220 \t loss: 7.245224404843398\n",
      "epoch: 240 \t loss: 7.5010603344473665\n",
      "epoch: 260 \t loss: 7.362404772786467\n",
      "epoch: 280 \t loss: 7.246106693038612\n",
      "epoch: 300 \t loss: 7.355949110062856\n",
      "epoch: 320 \t loss: 7.246750031695146\n",
      "epoch: 340 \t loss: 7.245581897029781\n",
      "epoch: 360 \t loss: 7.2483548453714945\n",
      "epoch: 380 \t loss: 7.244675918433139\n",
      "epoch: 400 \t loss: 7.244465761450252\n",
      "epoch: 420 \t loss: 7.241550142517366\n",
      "epoch: 440 \t loss: 7.241528724828559\n",
      "epoch: 460 \t loss: 7.241293078761627\n",
      "epoch: 480 \t loss: 7.24313799301105\n",
      "epoch: 500 \t loss: 7.2440219564586075\n",
      "epoch: 520 \t loss: 7.24338254324416\n",
      "epoch: 540 \t loss: 7.245070936221554\n",
      "epoch: 560 \t loss: 7.2444984282915685\n",
      "epoch: 580 \t loss: 7.245063642866469\n",
      "epoch: 600 \t loss: 7.243009614063824\n",
      "epoch: 620 \t loss: 7.246584104595895\n",
      "epoch: 640 \t loss: 7.244180127891949\n",
      "epoch: 660 \t loss: 7.243847532067243\n",
      "epoch: 680 \t loss: 7.243414447994771\n",
      "epoch: 700 \t loss: 7.246057649179559\n",
      "epoch: 720 \t loss: 7.244813203609689\n",
      "epoch: 740 \t loss: 7.241486410345686\n",
      "epoch: 760 \t loss: 7.240631450805651\n",
      "epoch: 780 \t loss: 7.244094362013095\n",
      "epoch: 800 \t loss: 7.243793965771619\n",
      "epoch: 820 \t loss: 7.2437559276542025\n",
      "epoch: 840 \t loss: 7.245242830419218\n",
      "epoch: 860 \t loss: 7.578203062220611\n",
      "epoch: 880 \t loss: 7.237669473469988\n",
      "epoch: 900 \t loss: 7.899292224007001\n",
      "epoch: 920 \t loss: 7.662021540315632\n",
      "epoch: 940 \t loss: 7.273172503335927\n",
      "epoch: 960 \t loss: 7.252309287882232\n",
      "epoch: 980 \t loss: 7.24827373556796\n",
      "Layer  3 , Pre-training ======================\n",
      "epoch: 0 \t loss: 7.21363262645889\n",
      "epoch: 20 \t loss: 7.201895634183614\n",
      "epoch: 40 \t loss: 7.1934439284470235\n",
      "epoch: 60 \t loss: 7.183007132537754\n",
      "epoch: 80 \t loss: 7.180463324287501\n",
      "epoch: 100 \t loss: 7.172422643236006\n",
      "epoch: 120 \t loss: 7.166275879764356\n",
      "epoch: 140 \t loss: 7.161248780723697\n",
      "epoch: 160 \t loss: 7.155636427851954\n",
      "epoch: 180 \t loss: 7.148808763699608\n",
      "epoch: 200 \t loss: 7.1450056109491475\n",
      "epoch: 220 \t loss: 7.135379592697824\n",
      "epoch: 240 \t loss: 7.132562586111095\n",
      "epoch: 260 \t loss: 7.121259917337763\n",
      "epoch: 280 \t loss: 7.125081741840384\n",
      "epoch: 300 \t loss: 7.1184236264236365\n",
      "epoch: 320 \t loss: 7.112233627343548\n",
      "epoch: 340 \t loss: 7.091426305943386\n",
      "epoch: 360 \t loss: 7.108033399416052\n",
      "epoch: 380 \t loss: 7.105518225221921\n",
      "epoch: 400 \t loss: 7.098173418757448\n",
      "epoch: 420 \t loss: 7.096480616632532\n",
      "epoch: 440 \t loss: 7.087774076214061\n",
      "epoch: 460 \t loss: 7.091035423835786\n",
      "epoch: 480 \t loss: 7.089900826809229\n",
      "Layer  3 , Full-training =====================\n",
      "epoch: 0 \t loss: 7.036677825435492\n",
      "epoch: 20 \t loss: 6.861406707324289\n",
      "epoch: 40 \t loss: 6.835233537185559\n",
      "epoch: 60 \t loss: 6.816929057991239\n",
      "epoch: 80 \t loss: 6.795699929511895\n",
      "epoch: 100 \t loss: 6.781945568751332\n",
      "epoch: 120 \t loss: 6.768367133735193\n",
      "epoch: 140 \t loss: 6.753471870466634\n",
      "epoch: 160 \t loss: 6.727839754947839\n",
      "epoch: 180 \t loss: 6.720715235412657\n",
      "epoch: 200 \t loss: 6.703894628010631\n",
      "epoch: 220 \t loss: 6.694231235941907\n",
      "epoch: 240 \t loss: 6.685673254293714\n",
      "epoch: 260 \t loss: 6.6817249590001495\n",
      "epoch: 280 \t loss: 6.657670861754696\n",
      "epoch: 300 \t loss: 6.665525849472518\n",
      "epoch: 320 \t loss: 6.664337859316503\n",
      "epoch: 340 \t loss: 6.653633341779705\n",
      "epoch: 360 \t loss: 6.654397748040318\n",
      "epoch: 380 \t loss: 6.694315908696658\n",
      "epoch: 400 \t loss: 6.647950730498249\n",
      "epoch: 420 \t loss: 6.646276933640125\n",
      "epoch: 440 \t loss: 6.686479215232138\n",
      "epoch: 460 \t loss: 6.633291498307349\n",
      "epoch: 480 \t loss: 6.641015482298912\n",
      "epoch: 500 \t loss: 6.632355856824713\n",
      "epoch: 520 \t loss: 6.639592801032005\n",
      "epoch: 540 \t loss: 6.6313683071323775\n",
      "epoch: 560 \t loss: 6.666188306147607\n",
      "epoch: 580 \t loss: 6.576836478130021\n",
      "epoch: 600 \t loss: 6.63267707470725\n",
      "epoch: 620 \t loss: 6.6377597059683895\n",
      "epoch: 640 \t loss: 6.637006035936104\n",
      "epoch: 660 \t loss: 6.637999186413112\n",
      "epoch: 680 \t loss: 6.626488175928263\n",
      "epoch: 700 \t loss: 6.853108796633319\n",
      "epoch: 720 \t loss: 6.632318137687605\n",
      "epoch: 740 \t loss: 6.666896881744095\n",
      "epoch: 760 \t loss: 6.633388848769451\n",
      "epoch: 780 \t loss: 6.643731727825027\n",
      "epoch: 800 \t loss: 6.639894940386111\n",
      "epoch: 820 \t loss: 6.636866491031153\n",
      "epoch: 840 \t loss: 6.629124223179022\n",
      "epoch: 860 \t loss: 6.636577371625092\n",
      "epoch: 880 \t loss: 6.647665870010116\n",
      "epoch: 900 \t loss: 6.635590450450267\n",
      "epoch: 920 \t loss: 6.619704932499731\n",
      "epoch: 940 \t loss: 7.602473047411068\n",
      "epoch: 960 \t loss: 7.472704217990832\n",
      "epoch: 980 \t loss: 7.415914389221082\n",
      "Layer  4 , Pre-training ======================\n",
      "epoch: 0 \t loss: 7.322777118604008\n",
      "epoch: 20 \t loss: 7.2957182114977375\n",
      "epoch: 40 \t loss: 7.268984024680625\n",
      "epoch: 60 \t loss: 7.219274537580899\n",
      "epoch: 80 \t loss: 7.233106282369451\n",
      "epoch: 100 \t loss: 7.208203426215156\n",
      "epoch: 120 \t loss: 6.98989253482436\n",
      "epoch: 140 \t loss: 7.17999245935451\n",
      "epoch: 160 \t loss: 7.159975819944617\n",
      "epoch: 180 \t loss: 7.154622013680556\n",
      "epoch: 200 \t loss: 7.140591958075389\n",
      "epoch: 220 \t loss: 7.123745502611955\n",
      "epoch: 240 \t loss: 7.072731301050188\n",
      "epoch: 260 \t loss: 7.121926351788987\n",
      "epoch: 280 \t loss: 6.936235294439723\n",
      "epoch: 300 \t loss: 7.112084422618582\n",
      "epoch: 320 \t loss: 7.108653025465064\n",
      "epoch: 340 \t loss: 7.104672266621832\n",
      "epoch: 360 \t loss: 7.09956594580254\n",
      "epoch: 380 \t loss: 7.095039333538721\n",
      "epoch: 400 \t loss: 7.090992191003116\n",
      "epoch: 420 \t loss: 7.09130272782169\n",
      "epoch: 440 \t loss: 7.082520153140054\n",
      "epoch: 460 \t loss: 7.081384238150261\n",
      "epoch: 480 \t loss: 5.483691825027591\n",
      "Layer  4 , Full-training =====================\n",
      "epoch: 0 \t loss: 7.029575999028167\n",
      "epoch: 20 \t loss: 6.892821675174592\n",
      "epoch: 40 \t loss: 6.831035172267093\n",
      "epoch: 60 \t loss: 6.8011258790733695\n",
      "epoch: 80 \t loss: 6.762671345948478\n",
      "epoch: 100 \t loss: 6.68674685571207\n",
      "epoch: 120 \t loss: 6.715154641007802\n",
      "epoch: 140 \t loss: 6.7387580107897795\n",
      "epoch: 160 \t loss: 6.707021686202718\n",
      "epoch: 180 \t loss: 6.704301944106303\n",
      "epoch: 200 \t loss: 6.674262361388269\n",
      "epoch: 220 \t loss: 6.663317357161727\n",
      "epoch: 240 \t loss: 6.665716314205139\n",
      "epoch: 260 \t loss: 6.7472438248322035\n",
      "epoch: 280 \t loss: 6.722753655188606\n",
      "epoch: 300 \t loss: 6.718748229218496\n",
      "epoch: 320 \t loss: 6.704200228966994\n",
      "epoch: 340 \t loss: 28.949532577567087\n",
      "epoch: 360 \t loss: 6.299429194422825\n",
      "epoch: 380 \t loss: 6.708119688379705\n",
      "epoch: 400 \t loss: 6.673734252907691\n",
      "epoch: 420 \t loss: 6.808415416926568\n",
      "epoch: 440 \t loss: 6.880087516366787\n",
      "epoch: 460 \t loss: 6.6804362670848985\n",
      "epoch: 480 \t loss: 6.579796998715358\n",
      "epoch: 500 \t loss: 6.824298617929746\n",
      "epoch: 520 \t loss: 6.599319772039085\n",
      "epoch: 540 \t loss: 29.38859564149491\n",
      "epoch: 560 \t loss: 7.891857384505946\n",
      "epoch: 580 \t loss: 7.636162084363381\n",
      "epoch: 600 \t loss: 7.798584999673512\n",
      "epoch: 620 \t loss: 7.9085989036069995\n",
      "epoch: 640 \t loss: 7.57497087237798\n",
      "epoch: 660 \t loss: 7.514346252167877\n",
      "epoch: 680 \t loss: 7.317807916237598\n",
      "epoch: 700 \t loss: 7.460845833123428\n",
      "epoch: 720 \t loss: 7.22787804010709\n",
      "epoch: 740 \t loss: 7.321971704424861\n",
      "epoch: 760 \t loss: 7.058231677550464\n",
      "epoch: 780 \t loss: 7.433997367470964\n",
      "epoch: 800 \t loss: 7.413833572227076\n",
      "epoch: 820 \t loss: 7.388222537873641\n",
      "epoch: 840 \t loss: 6.921760576687951\n",
      "epoch: 860 \t loss: 7.381104859033478\n",
      "epoch: 880 \t loss: 7.383413743098361\n",
      "epoch: 900 \t loss: 7.375487785296302\n",
      "epoch: 920 \t loss: 7.340707659660061\n",
      "epoch: 940 \t loss: 7.267371051100467\n",
      "epoch: 960 \t loss: 7.183747732112109\n",
      "epoch: 980 \t loss: 7.1655948361359885\n",
      "Training end. Time: 211.4621160030365\n"
     ]
    }
   ],
   "source": [
    "## =================Layerwise Training======================\n",
    "start = time.time()\n",
    "for stage in range(maxIt):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# in k-th stage, we train the k-th layer\n",
    "    \n",
    "\t## Pre-training: only train the k-th layer\n",
    "\tprint('Layer ',stage,', Pre-training ======================')\n",
    "\tif(stage > 6):\n",
    "\t\tNepoches_full = 500\n",
    "\tif(stage > 0):\n",
    "\t\toptimizers[stage].param_groups[0]['lr'] = net.ths_v[stage-1].data * lr_fac / 5000.0\n",
    "\tfor epoch in range(Nepoches_pre):\n",
    "\t\tfor i in range(maxIt):\n",
    "\t\t\toptimizers[i].zero_grad()\n",
    "    \n",
    "# \t\tU0_t,V0_t,Y0_t = generate_problem(r,d1,d2,alpha)\n",
    "\t\tnet.EnableSingleLayer(stage)\n",
    "\t\tif(stage > 0):\n",
    "\t\t\tnet.InitializeThs(stage)\n",
    "\t\tloss = net(Y0_t, r, X0_t, stage+1)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizers[stage].step()\n",
    "    \n",
    "\t\tif(epoch % 10 == 0):\n",
    "\t\t\tif net.CheckNegative():\n",
    "\t\t\t\tprint(\"Negative detected, restored\")\n",
    "\t\t\t\t\n",
    "\t\tlr = optimizers[stage].param_groups[0]['lr']\n",
    "\t\tif epoch % 20 == 0:\n",
    "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))\n",
    "\n",
    "\t# Full-training: train 0~k th layers\n",
    "\tprint('Layer ',stage,', Full-training =====================')\n",
    "\tif stage == 0:\n",
    "\t\tcontinue\n",
    "\n",
    "\tfor epoch in range(Nepoches_full):\n",
    "\t\tfor i in range(maxIt):\n",
    "\t\t\toptimizers[i].zero_grad()\n",
    "    \n",
    "# \t\tU0_t,V0_t,Y0_t = generate_problem(r,d1,d2,alpha)\n",
    "\t\tnet.EnableLayers(stage+1)\n",
    "\t\tloss = net(Y0_t, r, X0_t, stage+1)\n",
    "\t\tloss.backward()\n",
    "        \n",
    "\t\tfor i in range(stage+1):\n",
    "\t\t\toptimizers[i].step()\n",
    "\n",
    "\t\tif epoch % 20 == 0:\n",
    "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))\n",
    "            \n",
    "    \n",
    "end = time.time()\n",
    "print(\"Training end. Time: \" + str(end - start))\n",
    "\n",
    "## =====================Save model to .mat file ========================\n",
    "result_ths \t= np.zeros((maxIt,))\n",
    "result_stp1 = np.zeros((maxIt,))\n",
    "result_stp2 = np.zeros((maxIt,))\n",
    "for i in range(maxIt):\n",
    "\tresult_ths[i] \t= net.ths_v[i].data.cpu().numpy()\n",
    "\tresult_stp1[i] \t= net.step[i].data.cpu().numpy()\n",
    "\n",
    "spath = 'LRPCA'+'.mat'\n",
    "sio.savemat(spath, {'ths':result_ths, 'step':result_stp1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fa2ba5-2ae4-4e9a-b627-4beb1df3bee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd1451-31db-4732-92ad-0b60d056225d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82b207-bae7-46ca-b771-7292934846d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce603b-4731-4350-8af9-f5da95dfa065",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## =================Layerwise Training======================\n",
    "start = time.time()\n",
    "for stage in range(maxIt):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# in k-th stage, we train the k-th layer\n",
    "    \n",
    "\t## Pre-training: only train the k-th layer\n",
    "\tprint('Layer ', stage, ', Pre-training ======================')  \n",
    "\tif(stage > 6):\n",
    "\t\tNepoches_full = 500\n",
    "\tif(stage > 0):\n",
    "\t\toptimizers[stage].param_groups[0]['lr'] = net.ths_v[stage-1].data * lr_fac / 500.0\n",
    "    \n",
    "\tfor epoch in range(Nepoches_pre):\n",
    "\t\tfor i in range(maxIt):\n",
    "\t\t\toptimizers[i].zero_grad()\n",
    "   \n",
    "\t\tnet.EnableSingleLayer(stage)\n",
    "\t\tif(stage > 0):\n",
    "\t\t\tnet.InitializeThs(stage)\n",
    "\n",
    "\t\tloss = net(Y0_t, r, X0_t, stage+1)\n",
    "\t\tloss.backward()\n",
    "\t\tprint(net.ths_v[0].grad)\n",
    "\t\toptimizers[stage].step()\n",
    "        \n",
    "\t\tif(epoch % 10 == 0):\n",
    "\t\t\tif net.CheckNegative():\n",
    "\t\t\t\tprint(\"Negative detected, restored\")\n",
    "\t\t\tif net.CheckNa():\n",
    "\t\t\t\tprint(\"Na detected, restored\")\n",
    "                \n",
    "\t\tlr = optimizers[stage].param_groups[0]['lr']\n",
    "\t\tif epoch % 20 == 0:\n",
    "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))\n",
    "        \n",
    "\t# Full-training: train 0~k th layers\n",
    "\tprint('Layer ', stage,', Full-training =====================')\n",
    "\tif stage == 0:\n",
    "\t\tcontinue\n",
    "\tfor epoch in range(Nepoches_full):\n",
    "\t\tfor i in range(maxIt):\n",
    "\t\t\toptimizers[i].zero_grad()\n",
    "    \n",
    "\t\tnet.EnableLayers(stage+1)\n",
    "\t\tloss = net(Y0_t, r, X0_t, stage+1)\n",
    "\t\tloss.backward()\n",
    "        \n",
    "\t\tfor i in range(stage+1):\n",
    "\t\t\toptimizers[i].step()\n",
    "\n",
    "\t\tif epoch % 20 == 0:\n",
    "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))  \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training end. Time: \" + str(end - start))\n",
    "\n",
    "## =====================Save model to .mat file ========================\n",
    "result_ths \t= np.zeros((maxIt,))\n",
    "result_stp1 = np.zeros((maxIt,))\n",
    "result_stp2 = np.zeros((maxIt,))\n",
    "for i in range(maxIt):\n",
    "\tresult_ths[i] \t= net.ths_v[i].data.cpu().numpy()\n",
    "\tresult_stp1[i] \t= net.step[i].data.cpu().numpy()\n",
    "\n",
    "spath = 'LRPCA'+'.mat'\n",
    "sio.savemat(spath, {'ths':result_ths, 'step':result_stp1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45689702-4896-4b0d-a39f-9486264e16de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad856e49-118d-487a-9b78-dc1434a4c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
